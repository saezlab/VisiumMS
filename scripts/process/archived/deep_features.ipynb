{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch_env2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "kernel_name = os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))\n",
    "kernel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I am not conviced that I am extracting the image tiles in the correct way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/philipp/Work/VisiumMS/scripts/process/deep_features.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blserver/home/philipp/Work/VisiumMS/scripts/process/deep_features.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresnet50\u001b[39;00m \u001b[39mimport\u001b[39;00m ResNet50\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blserver/home/philipp/Work/VisiumMS/scripts/process/deep_features.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresnet50\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess_input \u001b[39mas\u001b[39;00m keras_preprocess_input\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as keras_preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import scanpy as sc\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "#model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.fc = torch.nn.Identity()  # remove last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]\n",
    "visium_dir = current_folder / \"..\" / \"..\" / \"data\" / \"uscsc_dump\"\n",
    "spot_images = current_folder / \"..\" / \"..\" / \"data\" / \"spot_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [f for f in os.listdir(visium_dir) if f.startswith(\"visium\")]\n",
    "{i: smp for i, smp in enumerate(samples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp = samples[3]\n",
    "print(smp)\n",
    "base_name = re.sub(r\"\\.h5ad$\", \"\", smp)\n",
    "base_name = re.sub(r\"^visium_\", \"\", base_name)\n",
    "vis_adata = sc.read_h5ad(visium_dir / smp)\n",
    "vis_adata.var_names_make_unique()\n",
    "print(\"Number of spots:\", vis_adata.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as keras_preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I am not sure whether I am reading in the images in the correct way\n",
    "# See here: https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "transform = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "features = np.zeros((len(vis_adata), 2048))\n",
    "#for i, img_name in enumerate(imgs):\n",
    "for i, barcode in enumerate(vis_adata.obs_names):\n",
    "    img_path = str(spot_images / base_name / (barcode + \".jpeg\"))\n",
    "\n",
    "    # see here: https://github.com/BiomedicalMachineLearning/stLearn/blob/09c8d7a79979268fe78273fcb25726c5e94c2c6c/stlearn/image_preprocessing/feature_extractor.py#L60\n",
    "    # and here: https://github.com/BiomedicalMachineLearning/stLearn/blob/09c8d7a79979268fe78273fcb25726c5e94c2c6c/stlearn/image_preprocessing/model_zoo.py#LL54C12-L54C12\n",
    "    # we need to make sure to use the same transforms as in the original training\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    features[i, :] = model.forward(img).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pca of the features\n",
    "pca_coords = PCA(n_components=50).fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first two components, color by vis_adata.obs.leiden using seaborn\n",
    "sns.scatterplot(x=pca_coords[:, 0], y=pca_coords[:, 1], hue=vis_adata.obs.leiden)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_adata.obs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make alpha = 0.2\n",
    "sns.scatterplot(x=pca_coords[:, 0], y=pca_coords[:, 1], hue=vis_adata.obs.p53, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make umap of the features\n",
    "umap_coords = umap.UMAP(n_components=2).fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot umap coords, color by vis_adata.obs.leiden using seaborn\n",
    "sns.scatterplot(x=umap_coords[:, 0], y=umap_coords[:, 1], hue=vis_adata.obs.leiden, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode = vis_adata.obs_names[0]\n",
    "img_path = str(spot_images / base_name / (barcode + \".jpeg\"))\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = Image.fromarray(img)\n",
    "img = img.resize((224, 224))\n",
    "img = np.array(img)\n",
    "img = np.transpose(img, (2, 0, 1))\n",
    "img = img / 255\n",
    "img = torch.from_numpy(img).float()\n",
    "img = img.unsqueeze(0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode = vis_adata.obs_names[0]\n",
    "img_path = str(spot_images / base_name / (barcode + \".jpeg\"))\n",
    "img = Image.open(img_path)\n",
    "img = np.asarray(img, dtype=\"int32\")\n",
    "img = img.astype(np.float32)\n",
    "img = np.stack([img])\n",
    "\n",
    "# The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling.\n",
    "\n",
    "img = np.transpose(img, (0, 3, 1, 2))\n",
    "img = torch.from_numpy(img).float()\n",
    "model.forward(img).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode = vis_adata.obs_names[10]\n",
    "img_path = str(spot_images / base_name / (barcode + \".jpeg\"))\n",
    "img = Image.open(img_path)\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
